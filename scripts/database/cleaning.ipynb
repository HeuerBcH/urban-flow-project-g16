{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebb4531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beed98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir caminho base do projeto\n",
    "import os\n",
    "\n",
    "# Detectar o diretório do projeto (onde está o README.md)\n",
    "current_dir = os.getcwd()\n",
    "print(f\"Diretório atual: {current_dir}\")\n",
    "\n",
    "# Procurar pelo diretório que contém o README.md\n",
    "project_root = current_dir\n",
    "while project_root != os.path.dirname(project_root):  # Enquanto não chegou na raiz\n",
    "    if os.path.exists(os.path.join(project_root, \"README.md\")):\n",
    "        break\n",
    "    project_root = os.path.dirname(project_root)\n",
    "\n",
    "# Se não encontrou, usar o diretório atual e subir 2 níveis (scripts/database -> scripts -> projeto)\n",
    "if not os.path.exists(os.path.join(project_root, \"README.md\")):\n",
    "    project_root = os.path.dirname(os.path.dirname(current_dir))\n",
    "\n",
    "data_raw_path = os.path.join(project_root, \"data\", \"raw\")\n",
    "\n",
    "print(f\"Diretório do projeto: {project_root}\")\n",
    "print(f\"Diretório de dados brutos: {data_raw_path}\")\n",
    "print(f\"README.md existe: {os.path.exists(os.path.join(project_root, 'README.md'))}\")\n",
    "print(f\"Pasta data/raw existe: {os.path.exists(data_raw_path)}\")\n",
    "\n",
    "# Listar arquivos na pasta data/raw para verificar\n",
    "if os.path.exists(data_raw_path):\n",
    "    print(f\"Arquivos em data/raw: {os.listdir(data_raw_path)}\")\n",
    "else:\n",
    "    print(\"[ERRO] Pasta data/raw nao encontrada!\")\n",
    "\n",
    "# Carregar os datasets disponíveis\n",
    "print(\"Carregando datasets disponíveis...\")\n",
    "\n",
    "# Lista de arquivos esperados e suas configurações\n",
    "arquivos_esperados = {\n",
    "    \"lista-de-semaforos.csv\": {\"sep\": \",\", \"encoding\": \"utf-8\"},\n",
    "    \"equipamentos-de-medicao-de-velocidade.csv\": {\"sep\": \",\", \"encoding\": \"utf-8\"},\n",
    "    \"fluxo-veiculos-hora-janeiro.csv\": {\"sep\": \";\", \"encoding\": \"utf-8\"},\n",
    "    \"fluxo-velocidade-em-quinze-minutos-foto-jan-2025.csv\": {\"sep\": \";\", \"encoding\": \"utf-8\"},\n",
    "    \"monitoramento-cttu.csv\": {\"sep\": \";\", \"encoding\": \"utf-8\"},\n",
    "    \"relatorio-fluxo-de-15-em-15-minutos-lomb-jan-25.csv\": {\"sep\": \";\", \"encoding\": \"utf-8\"},\n",
    "    \"relatorio-fluxo-de-15-em-15-minutos-lomb-fev-25.csv\": {\"sep\": \";\", \"encoding\": \"utf-8\"},\n",
    "    \"relatorio-fluxo-de-15-em-15-minutos-lomb-mar-25.csv\": {\"sep\": \";\", \"encoding\": \"utf-8\"},\n",
    "    \"relatorio-fluxo-de-15-em-15-minutos-lomb-abr-25.csv\": {\"sep\": \";\", \"encoding\": \"utf-8\"},\n",
    "    \"relatorio-fluxo-de-15-em-15-minutos-lomb-mai-25.csv\": {\"sep\": \";\", \"encoding\": \"utf-8\"},\n",
    "    \"relatorio-fluxo-de-15-em-15-minutos-lomb-jun-25.csv\": {\"sep\": \";\", \"encoding\": \"utf-8\"},\n",
    "    \"relatorio-fluxo-de-15-em-15-minutos-lomb-jul-25.csv\": {\"sep\": \";\", \"encoding\": \"utf-8\"},\n",
    "    \"relatorio-fluxo-de-15-em-15-minutos-lomb-ago-25.csv\": {\"sep\": \";\", \"encoding\": \"utf-8\"}\n",
    "}\n",
    "\n",
    "# Carregar apenas os arquivos que existem\n",
    "datasets_carregados = {}\n",
    "for arquivo, config in arquivos_esperados.items():\n",
    "    caminho_arquivo = os.path.join(data_raw_path, arquivo)\n",
    "    if os.path.exists(caminho_arquivo):\n",
    "        try:\n",
    "            df = pd.read_csv(caminho_arquivo, **config)\n",
    "            datasets_carregados[arquivo] = df\n",
    "            print(f\"[OK] {arquivo} carregado ({df.shape[0]} registros)\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERRO] Erro ao carregar {arquivo}: {e}\")\n",
    "    else:\n",
    "        print(f\"[AVISO] {arquivo} nao encontrado\")\n",
    "\n",
    "# Atribuir aos DataFrames principais (apenas os que existem)\n",
    "semaforos_df = datasets_carregados.get(\"lista-de-semaforos.csv\")\n",
    "equip_med_vel_df = datasets_carregados.get(\"equipamentos-de-medicao-de-velocidade.csv\")\n",
    "\n",
    "print(f\"\\nTotal de datasets carregados: {len(datasets_carregados)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4603abe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os datasets restantes\n",
    "print(\"Carregando datasets restantes...\")\n",
    "\n",
    "# Atribuir os datasets restantes\n",
    "fluxo_veiculos_hora_df = datasets_carregados.get(\"fluxo-veiculos-hora-janeiro.csv\")\n",
    "fluxo_velocidade_15min_df = datasets_carregados.get(\"fluxo-velocidade-em-quinze-minutos-foto-jan-2025.csv\")\n",
    "monitoramento_cttu_df = datasets_carregados.get(\"monitoramento-cttu.csv\")\n",
    "relatorio_fluxo_jan_df = datasets_carregados.get(\"relatorio-fluxo-de-15-em-15-minutos-lomb-jan-25.csv\")\n",
    "relatorio_fluxo_fev_df = datasets_carregados.get(\"relatorio-fluxo-de-15-em-15-minutos-lomb-fev-25.csv\")\n",
    "relatorio_fluxo_mar_df = datasets_carregados.get(\"relatorio-fluxo-de-15-em-15-minutos-lomb-mar-25.csv\")\n",
    "relatorio_fluxo_abr_df = datasets_carregados.get(\"relatorio-fluxo-de-15-em-15-minutos-lomb-abr-25.csv\")\n",
    "relatorio_fluxo_mai_df = datasets_carregados.get(\"relatorio-fluxo-de-15-em-15-minutos-lomb-mai-25.csv\")\n",
    "relatorio_fluxo_jun_df = datasets_carregados.get(\"relatorio-fluxo-de-15-em-15-minutos-lomb-jun-25.csv\")\n",
    "relatorio_fluxo_jul_df = datasets_carregados.get(\"relatorio-fluxo-de-15-em-15-minutos-lomb-jul-25.csv\")\n",
    "relatorio_fluxo_ago_df = datasets_carregados.get(\"relatorio-fluxo-de-15-em-15-minutos-lomb-ago-25.csv\")\n",
    "\n",
    "\n",
    "# Verificar quais datasets foram carregados com sucesso\n",
    "datasets_info = {\n",
    "    \"Semáforos\": semaforos_df,\n",
    "    \"Equipamentos de Medição de Velocidade\": equip_med_vel_df,\n",
    "    \"Fluxo de Veículos por Hora\": fluxo_veiculos_hora_df,\n",
    "    \"Fluxo e Velocidade 15min\": fluxo_velocidade_15min_df,\n",
    "    \"Monitoramento CTTU\": monitoramento_cttu_df,\n",
    "    \"Relatório Fluxo Agosto\": relatorio_fluxo_ago_df,\n",
    "    \"Relatório Fluxo Fevereiro\": relatorio_fluxo_fev_df\n",
    "}\n",
    "\n",
    "print(\"\\nStatus dos datasets:\")\n",
    "for nome, df in datasets_info.items():\n",
    "    if df is not None:\n",
    "        print(f\"[OK] {nome}: {df.shape[0]} registros\")\n",
    "    else:\n",
    "        print(f\"[ERRO] {nome}: Nao disponivel\")\n",
    "\n",
    "print(f\"\\nTotal de datasets disponíveis: {sum(1 for df in datasets_info.values() if df is not None)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b66057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise exploratória inicial dos datasets\n",
    "print(\"=== ANÁLISE EXPLORATÓRIA DOS DATASETS ===\\n\")\n",
    "\n",
    "datasets = {\n",
    "    \"Semáforos\": semaforos_df,\n",
    "    \"Equipamentos de Medição de Velocidade\": equip_med_vel_df,\n",
    "    \"Fluxo de Veículos por Hora\": fluxo_veiculos_hora_df,\n",
    "    \"Fluxo e Velocidade 15min\": fluxo_velocidade_15min_df,\n",
    "    \"Monitoramento CTTU\": monitoramento_cttu_df,\n",
    "    \"Relatório Fluxo Janeiro\": relatorio_fluxo_jan_df,\n",
    "    \"Relatório Fluxo Fevereiro\": relatorio_fluxo_fev_df,\n",
    "    \"Relatório Fluxo Março\": relatorio_fluxo_mar_df,\n",
    "    \"Relatório Fluxo Abril\": relatorio_fluxo_abr_df,\n",
    "    \"Relatório Fluxo Maio\": relatorio_fluxo_mai_df,\n",
    "    \"Relatório Fluxo Junho\": relatorio_fluxo_jun_df,\n",
    "    \"Relatório Fluxo Julho\": relatorio_fluxo_jul_df,\n",
    "    \"Relatório Fluxo Agosto\": relatorio_fluxo_ago_df\n",
    "}\n",
    "\n",
    "for nome, df in datasets.items():\n",
    "    print(f\"[INFO] {nome}:\")\n",
    "    if df is not None:\n",
    "        print(f\"   - Dimensões: {df.shape[0]} linhas x {df.shape[1]} colunas\")\n",
    "        print(f\"   - Colunas: {list(df.columns)}\")\n",
    "        print(f\"   - Tipos de dados:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"     * {col}: {df[col].dtype}\")\n",
    "        print(f\"   - Valores nulos: {df.isnull().sum().sum()}\")\n",
    "        print(f\"   - Memória: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    else:\n",
    "        print(\"   - Status: Não disponível\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48acdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIMPEZA E PADRONIZAÇÃO DOS DADOS\n",
    "\n",
    "print(\"=== INICIANDO LIMPEZA DOS DADOS ===\\n\")\n",
    "\n",
    "# 1. LIMPEZA DOS SEMÁFOROS\n",
    "if semaforos_df is not None:\n",
    "    print(\"[PROCESSANDO] Limpando dados de semaforos...\")\n",
    "    semaforos_clean = semaforos_df.copy()\n",
    "\n",
    "    # Padronizar coordenadas\n",
    "    semaforos_clean['latitude'] = pd.to_numeric(semaforos_clean['latitude'], errors='coerce')\n",
    "    semaforos_clean['longitude'] = pd.to_numeric(semaforos_clean['longitude'], errors='coerce')\n",
    "\n",
    "    # Limpar strings (apenas se forem do tipo object/string)\n",
    "    if 'localizacao1' in semaforos_clean.columns and semaforos_clean['localizacao1'].dtype == 'object':\n",
    "        semaforos_clean['localizacao1'] = semaforos_clean['localizacao1'].astype(str).str.strip()\n",
    "    if 'localizacao2' in semaforos_clean.columns and semaforos_clean['localizacao2'].dtype == 'object':\n",
    "        semaforos_clean['localizacao2'] = semaforos_clean['localizacao2'].astype(str).str.strip()\n",
    "    if 'bairro' in semaforos_clean.columns and semaforos_clean['bairro'].dtype == 'object':\n",
    "        semaforos_clean['bairro'] = semaforos_clean['bairro'].astype(str).str.strip().str.upper()\n",
    "\n",
    "    # Adicionar coluna de identificação única\n",
    "    semaforos_clean['id_semaforo'] = semaforos_clean['_id']\n",
    "\n",
    "    print(f\"   [OK] Semaforos limpos: {semaforos_clean.shape[0]} registros\")\n",
    "    print(f\"   [OK] Valores nulos: {semaforos_clean.isnull().sum().sum()}\")\n",
    "else:\n",
    "    print(\"[AVISO] Semaforos nao disponivel - pulando limpeza\")\n",
    "    semaforos_clean = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920f105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. LIMPEZA DOS EQUIPAMENTOS DE MEDIÇÃO DE VELOCIDADE\n",
    "if equip_med_vel_df is not None:\n",
    "    print(\"[CONFIGURANDO] Limpando dados de equipamentos de medição de velocidade...\")\n",
    "    equip_med_vel_clean = equip_med_vel_df.copy()\n",
    "    \n",
    "    # Mostrar colunas disponíveis para debug\n",
    "    print(f\"   Colunas disponíveis: {list(equip_med_vel_clean.columns)}\")\n",
    "\n",
    "    # Padronizar coordenadas\n",
    "    equip_med_vel_clean['latitude'] = pd.to_numeric(equip_med_vel_clean['latitude'], errors='coerce')\n",
    "    equip_med_vel_clean['longitude'] = pd.to_numeric(equip_med_vel_clean['longitude'], errors='coerce')\n",
    "\n",
    "    # Padronizar valores numéricos (apenas se as colunas existirem)\n",
    "    if 'faixas_fiscalizadas' in equip_med_vel_clean.columns:\n",
    "        equip_med_vel_clean['faixas_fiscalizadas'] = pd.to_numeric(equip_med_vel_clean['faixas_fiscalizadas'], errors='coerce')\n",
    "    if 'velocidade_fiscalizada' in equip_med_vel_clean.columns:\n",
    "        equip_med_vel_clean['velocidade_fiscalizada'] = pd.to_numeric(equip_med_vel_clean['velocidade_fiscalizada'], errors='coerce')\n",
    "    if 'vmd' in equip_med_vel_clean.columns:\n",
    "        equip_med_vel_clean['vmd'] = pd.to_numeric(equip_med_vel_clean['vmd'], errors='coerce')\n",
    "\n",
    "    # Limpar strings (apenas se as colunas existirem e forem do tipo object/string)\n",
    "    if 'local_instalacao' in equip_med_vel_clean.columns and equip_med_vel_clean['local_instalacao'].dtype == 'object':\n",
    "        equip_med_vel_clean['local_instalacao'] = equip_med_vel_clean['local_instalacao'].astype(str).str.strip()\n",
    "    if 'sentido_fiscalizacao' in equip_med_vel_clean.columns and equip_med_vel_clean['sentido_fiscalizacao'].dtype == 'object':\n",
    "        equip_med_vel_clean['sentido_fiscalizacao'] = equip_med_vel_clean['sentido_fiscalizacao'].astype(str).str.strip()\n",
    "    if 'periodo_vmd' in equip_med_vel_clean.columns and equip_med_vel_clean['periodo_vmd'].dtype == 'object':\n",
    "        equip_med_vel_clean['periodo_vmd'] = equip_med_vel_clean['periodo_vmd'].astype(str).str.strip()\n",
    "\n",
    "    print(f\"   [OK] Equipamentos de medição limpos: {equip_med_vel_clean.shape[0]} registros\")\n",
    "    print(f\"   [OK] Valores nulos: {equip_med_vel_clean.isnull().sum().sum()}\")\n",
    "else:\n",
    "    print(\"[ATENCAO]  Equipamentos de medição não disponível - pulando limpeza\")\n",
    "    equip_med_vel_clean = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a637dbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. LIMPEZA DO FLUXO DE VEÍCULOS POR HORA\n",
    "if fluxo_veiculos_hora_df is not None:\n",
    "    print(\"[CONFIGURANDO] Limpando dados de fluxo de veículos por hora...\")\n",
    "    fluxo_veiculos_hora_clean = fluxo_veiculos_hora_df.copy()\n",
    "\n",
    "    # Converter colunas de quantidade para numérico\n",
    "    quantidade_cols = [col for col in fluxo_veiculos_hora_clean.columns if col.startswith('quant')]\n",
    "    for col in quantidade_cols:\n",
    "        fluxo_veiculos_hora_clean[col] = pd.to_numeric(fluxo_veiculos_hora_clean[col], errors='coerce')\n",
    "\n",
    "    # Converter colunas de porcentagem para numérico (substituir vírgula por ponto)\n",
    "    porcentagem_cols = [col for col in fluxo_veiculos_hora_clean.columns if col.startswith('porcentagem')]\n",
    "    for col in porcentagem_cols:\n",
    "        fluxo_veiculos_hora_clean[col] = fluxo_veiculos_hora_clean[col].astype(str).str.replace(',', '.').astype(float)\n",
    "\n",
    "    # Limpar strings (apenas se forem do tipo object/string)\n",
    "    if 'equipamento' in fluxo_veiculos_hora_clean.columns and fluxo_veiculos_hora_clean['equipamento'].dtype == 'object':\n",
    "        fluxo_veiculos_hora_clean['equipamento'] = fluxo_veiculos_hora_clean['equipamento'].astype(str).str.strip()\n",
    "    if 'logradouro' in fluxo_veiculos_hora_clean.columns and fluxo_veiculos_hora_clean['logradouro'].dtype == 'object':\n",
    "        fluxo_veiculos_hora_clean['logradouro'] = fluxo_veiculos_hora_clean['logradouro'].astype(str).str.strip()\n",
    "\n",
    "    # Converter horários para datetime\n",
    "    fluxo_veiculos_hora_clean['horainicio'] = pd.to_datetime(fluxo_veiculos_hora_clean['horainicio'], format='%H:%M', errors='coerce')\n",
    "    fluxo_veiculos_hora_clean['horafinal'] = pd.to_datetime(fluxo_veiculos_hora_clean['horafinal'], format='%H:%M', errors='coerce')\n",
    "\n",
    "    print(f\"   [OK] Fluxo de veículos por hora limpo: {fluxo_veiculos_hora_clean.shape[0]} registros\")\n",
    "    print(f\"   [OK] Valores nulos: {fluxo_veiculos_hora_clean.isnull().sum().sum()}\")\n",
    "else:\n",
    "    print(\"[ATENCAO]  Fluxo de veículos por hora não disponível - pulando limpeza\")\n",
    "    fluxo_veiculos_hora_clean = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c5aa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. LIMPEZA DO FLUXO E VELOCIDADE EM 15 MINUTOS\n",
    "if fluxo_velocidade_15min_df is not None:\n",
    "    print(\"[CONFIGURANDO] Limpando dados de fluxo e velocidade em 15 minutos...\")\n",
    "    fluxo_velocidade_15min_clean = fluxo_velocidade_15min_df.copy()\n",
    "    \n",
    "    # CORREÇÃO: O arquivo tem colunas trocadas no cabeçalho\n",
    "    # Renomear colunas para corrigir a estrutura\n",
    "    print(\"   [ATUALIZANDO] Corrigindo estrutura das colunas...\")\n",
    "    \n",
    "    # Primeiro, remover a coluna minutos_intervalo original se existir (para evitar duplicata após renomeação)\n",
    "    if 'minutos_intervalo' in fluxo_velocidade_15min_clean.columns:\n",
    "        fluxo_velocidade_15min_clean = fluxo_velocidade_15min_clean.drop('minutos_intervalo', axis=1)\n",
    "    fluxo_velocidade_15min_clean = fluxo_velocidade_15min_clean.rename(columns={\n",
    "        'mes': 'equipamento',           # mes contém códigos de equipamento\n",
    "        'equipamento': 'faixa',         # equipamento contém números de faixa\n",
    "        'faixa': 'data',                # faixa contém datas\n",
    "        'data': 'hora',                 # data contém horas\n",
    "        'hora': 'minutos_intervalo'     # hora contém intervalos de minutos\n",
    "    })\n",
    "    \n",
    "    # Adicionar coluna mes correta (extrair do equipamento ou usar valor padrão)\n",
    "    fluxo_velocidade_15min_clean['mes'] = 1  # Janeiro 2025\n",
    "    \n",
    "    # Garantir que não há colunas duplicadas (remover duplicatas por nome)\n",
    "    fluxo_velocidade_15min_clean = fluxo_velocidade_15min_clean.loc[:, ~fluxo_velocidade_15min_clean.columns.duplicated()]\n",
    "    \n",
    "    # Remover coluna duplicada se existir\n",
    "    if 'minutos_intervalo.1' in fluxo_velocidade_15min_clean.columns:\n",
    "        fluxo_velocidade_15min_clean = fluxo_velocidade_15min_clean.drop('minutos_intervalo.1', axis=1)\n",
    "        print(\"   [ATUALIZANDO] Coluna duplicada removida\")\n",
    "    \n",
    "    # Mostrar colunas corrigidas\n",
    "    print(f\"   Colunas corrigidas: {list(fluxo_velocidade_15min_clean.columns)}\")\n",
    "    \n",
    "    # Converter colunas de quantidade para numérico\n",
    "    quantidade_cols = [col for col in fluxo_velocidade_15min_clean.columns if col.startswith('qtd_')]\n",
    "    for col in quantidade_cols:\n",
    "        fluxo_velocidade_15min_clean[col] = pd.to_numeric(fluxo_velocidade_15min_clean[col], errors='coerce')\n",
    "    \n",
    "    # Preencher valores NaN com 0 nas colunas de quantidade\n",
    "    for col in quantidade_cols:\n",
    "        fluxo_velocidade_15min_clean[col] = fluxo_velocidade_15min_clean[col].fillna(0)\n",
    "\n",
    "    # Converter data para datetime\n",
    "    if 'data' in fluxo_velocidade_15min_clean.columns:\n",
    "        fluxo_velocidade_15min_clean['data'] = pd.to_datetime(fluxo_velocidade_15min_clean['data'], errors='coerce')\n",
    "\n",
    "    # Limpar strings (apenas se forem do tipo object/string)\n",
    "    if 'equipamento' in fluxo_velocidade_15min_clean.columns and fluxo_velocidade_15min_clean['equipamento'].dtype == 'object':\n",
    "        fluxo_velocidade_15min_clean['equipamento'] = fluxo_velocidade_15min_clean['equipamento'].astype(str).str.strip()\n",
    "    if 'faixa' in fluxo_velocidade_15min_clean.columns and fluxo_velocidade_15min_clean['faixa'].dtype == 'object':\n",
    "        fluxo_velocidade_15min_clean['faixa'] = fluxo_velocidade_15min_clean['faixa'].astype(str).str.strip()\n",
    "\n",
    "    # Converter hora para int\n",
    "    if 'hora' in fluxo_velocidade_15min_clean.columns:\n",
    "        fluxo_velocidade_15min_clean['hora'] = pd.to_numeric(fluxo_velocidade_15min_clean['hora'], errors='coerce')\n",
    "\n",
    "    print(f\"   [OK] Fluxo e velocidade 15min limpo: {fluxo_velocidade_15min_clean.shape[0]} registros\")\n",
    "    print(f\"   [OK] Valores nulos: {fluxo_velocidade_15min_clean.isnull().sum().sum()}\")\n",
    "else:\n",
    "    print(\"[ATENCAO]  Fluxo e velocidade 15min não disponível - pulando limpeza\")\n",
    "    fluxo_velocidade_15min_clean = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730eb8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. LIMPEZA DOS RELATÓRIOS DE FLUXO (AGOSTO E FEVEREIRO)\n",
    "print(\"[CONFIGURANDO] Limpando dados dos relatórios de fluxo...\")\n",
    "\n",
    "# Limpeza do relatório de Janeiro\n",
    "if relatorio_fluxo_jan_df is not None:\n",
    "    relatorio_fluxo_jan_clean = relatorio_fluxo_jan_df.copy()\n",
    "    quantidade_cols = [col for col in relatorio_fluxo_jan_clean.columns if col.startswith('qtd_')]\n",
    "    for col in quantidade_cols:\n",
    "        relatorio_fluxo_jan_clean[col] = pd.to_numeric(relatorio_fluxo_jan_clean[col], errors='coerce')\n",
    "\n",
    "    if 'data' in relatorio_fluxo_jan_clean.columns:\n",
    "        relatorio_fluxo_jan_clean['data'] = pd.to_datetime(relatorio_fluxo_jan_clean['data'], errors='coerce')\n",
    "    if 'equipamento' in relatorio_fluxo_jan_clean.columns and relatorio_fluxo_jan_clean['equipamento'].dtype == 'object':\n",
    "        relatorio_fluxo_jan_clean['equipamento'] = relatorio_fluxo_jan_clean['equipamento'].astype(str).str.strip()\n",
    "    if 'faixa' in relatorio_fluxo_jan_clean.columns and relatorio_fluxo_jan_clean['faixa'].dtype == 'object':\n",
    "        relatorio_fluxo_jan_clean['faixa'] = relatorio_fluxo_jan_clean['faixa'].astype(str).str.strip()\n",
    "    if 'hora' in relatorio_fluxo_jan_clean.columns:\n",
    "        relatorio_fluxo_jan_clean['hora'] = pd.to_numeric(relatorio_fluxo_jan_clean['hora'], errors='coerce')\n",
    "    print(f\"   [OK] Relatório Janeiro limpo: {relatorio_fluxo_jan_clean.shape[0]} registros\")\n",
    "else:\n",
    "    relatorio_fluxo_jan_clean = None\n",
    "    print(\"   [ATENCAO]  Relatório Janeiro não disponível\")\n",
    "\n",
    "# Limpeza do relatório de Fevereiro\n",
    "if relatorio_fluxo_fev_df is not None:\n",
    "    relatorio_fluxo_fev_clean = relatorio_fluxo_fev_df.copy()\n",
    "    quantidade_cols = [col for col in relatorio_fluxo_fev_clean.columns if col.startswith('qtd_')]\n",
    "    for col in quantidade_cols:\n",
    "        relatorio_fluxo_fev_clean[col] = pd.to_numeric(relatorio_fluxo_fev_clean[col], errors='coerce')\n",
    "\n",
    "    if 'data' in relatorio_fluxo_fev_clean.columns:\n",
    "        relatorio_fluxo_fev_clean['data'] = pd.to_datetime(relatorio_fluxo_fev_clean['data'], errors='coerce')\n",
    "    if 'equipamento' in relatorio_fluxo_fev_clean.columns and relatorio_fluxo_fev_clean['equipamento'].dtype == 'object':\n",
    "        relatorio_fluxo_fev_clean['equipamento'] = relatorio_fluxo_fev_clean['equipamento'].astype(str).str.strip()\n",
    "    if 'faixa' in relatorio_fluxo_fev_clean.columns and relatorio_fluxo_fev_clean['faixa'].dtype == 'object':\n",
    "        relatorio_fluxo_fev_clean['faixa'] = relatorio_fluxo_fev_clean['faixa'].astype(str).str.strip()\n",
    "    if 'hora' in relatorio_fluxo_fev_clean.columns:\n",
    "        relatorio_fluxo_fev_clean['hora'] = pd.to_numeric(relatorio_fluxo_fev_clean['hora'], errors='coerce')\n",
    "    print(f\"   [OK] Relatório Fevereiro limpo: {relatorio_fluxo_fev_clean.shape[0]} registros\")\n",
    "else:\n",
    "    relatorio_fluxo_fev_clean = None\n",
    "    print(\"   [ATENCAO]  Relatório Fevereiro não disponível\")\n",
    "\n",
    "# Limpeza do relatório de Março\n",
    "if relatorio_fluxo_mar_df is not None:\n",
    "    relatorio_fluxo_mar_clean = relatorio_fluxo_mar_df.copy()\n",
    "    quantidade_cols = [col for col in relatorio_fluxo_mar_clean.columns if col.startswith('qtd_')]\n",
    "    for col in quantidade_cols:\n",
    "        relatorio_fluxo_mar_clean[col] = pd.to_numeric(relatorio_fluxo_mar_clean[col], errors='coerce')\n",
    "\n",
    "    if 'data' in relatorio_fluxo_mar_clean.columns:\n",
    "        relatorio_fluxo_mar_clean['data'] = pd.to_datetime(relatorio_fluxo_mar_clean['data'], errors='coerce')\n",
    "    if 'equipamento' in relatorio_fluxo_mar_clean.columns and relatorio_fluxo_mar_clean['equipamento'].dtype == 'object':\n",
    "        relatorio_fluxo_mar_clean['equipamento'] = relatorio_fluxo_mar_clean['equipamento'].astype(str).str.strip()\n",
    "    if 'faixa' in relatorio_fluxo_mar_clean.columns and relatorio_fluxo_mar_clean['faixa'].dtype == 'object':\n",
    "        relatorio_fluxo_mar_clean['faixa'] = relatorio_fluxo_mar_clean['faixa'].astype(str).str.strip()\n",
    "    if 'hora' in relatorio_fluxo_mar_clean.columns:\n",
    "        relatorio_fluxo_mar_clean['hora'] = pd.to_numeric(relatorio_fluxo_mar_clean['hora'], errors='coerce')\n",
    "    print(f\"   [OK] Relatório Março limpo: {relatorio_fluxo_mar_clean.shape[0]} registros\")\n",
    "else:\n",
    "    relatorio_fluxo_mar_clean = None\n",
    "    print(\"   [ATENCAO]  Relatório Março não disponível\")\n",
    "\n",
    "# Limpeza do relatório de Abril\n",
    "if relatorio_fluxo_abr_df is not None:\n",
    "    relatorio_fluxo_abr_clean = relatorio_fluxo_abr_df.copy()\n",
    "    quantidade_cols = [col for col in relatorio_fluxo_abr_clean.columns if col.startswith('qtd_')]\n",
    "    for col in quantidade_cols:\n",
    "        relatorio_fluxo_abr_clean[col] = pd.to_numeric(relatorio_fluxo_abr_clean[col], errors='coerce')\n",
    "\n",
    "    if 'data' in relatorio_fluxo_abr_clean.columns:\n",
    "        relatorio_fluxo_abr_clean['data'] = pd.to_datetime(relatorio_fluxo_abr_clean['data'], errors='coerce')\n",
    "    if 'equipamento' in relatorio_fluxo_abr_clean.columns and relatorio_fluxo_abr_clean['equipamento'].dtype == 'object':\n",
    "        relatorio_fluxo_abr_clean['equipamento'] = relatorio_fluxo_abr_clean['equipamento'].astype(str).str.strip()\n",
    "    if 'faixa' in relatorio_fluxo_abr_clean.columns and relatorio_fluxo_abr_clean['faixa'].dtype == 'object':\n",
    "        relatorio_fluxo_abr_clean['faixa'] = relatorio_fluxo_abr_clean['faixa'].astype(str).str.strip()\n",
    "    if 'hora' in relatorio_fluxo_abr_clean.columns:\n",
    "        relatorio_fluxo_abr_clean['hora'] = pd.to_numeric(relatorio_fluxo_abr_clean['hora'], errors='coerce')\n",
    "    print(f\"   [OK] Relatório Abril limpo: {relatorio_fluxo_abr_clean.shape[0]} registros\")\n",
    "else:\n",
    "    relatorio_fluxo_abr_clean = None\n",
    "    print(\"   [ATENCAO]  Relatório Abril não disponível\")\n",
    "\n",
    "# Limpeza do relatório de Maio\n",
    "if relatorio_fluxo_mai_df is not None:\n",
    "    relatorio_fluxo_mai_clean = relatorio_fluxo_mai_df.copy()\n",
    "    quantidade_cols = [col for col in relatorio_fluxo_mai_clean.columns if col.startswith('qtd_')]\n",
    "    for col in quantidade_cols:\n",
    "        relatorio_fluxo_mai_clean[col] = pd.to_numeric(relatorio_fluxo_mai_clean[col], errors='coerce')\n",
    "\n",
    "    if 'data' in relatorio_fluxo_mai_clean.columns:\n",
    "        relatorio_fluxo_mai_clean['data'] = pd.to_datetime(relatorio_fluxo_mai_clean['data'], errors='coerce')\n",
    "    if 'equipamento' in relatorio_fluxo_mai_clean.columns and relatorio_fluxo_mai_clean['equipamento'].dtype == 'object':\n",
    "        relatorio_fluxo_mai_clean['equipamento'] = relatorio_fluxo_mai_clean['equipamento'].astype(str).str.strip()\n",
    "    if 'faixa' in relatorio_fluxo_mai_clean.columns and relatorio_fluxo_mai_clean['faixa'].dtype == 'object':\n",
    "        relatorio_fluxo_mai_clean['faixa'] = relatorio_fluxo_mai_clean['faixa'].astype(str).str.strip()\n",
    "    if 'hora' in relatorio_fluxo_mai_clean.columns:\n",
    "        relatorio_fluxo_mai_clean['hora'] = pd.to_numeric(relatorio_fluxo_mai_clean['hora'], errors='coerce')\n",
    "    print(f\"   [OK] Relatório Maio limpo: {relatorio_fluxo_mai_clean.shape[0]} registros\")\n",
    "else:\n",
    "    relatorio_fluxo_mai_clean = None\n",
    "    print(\"   [ATENCAO]  Relatório Maio não disponível\")\n",
    "\n",
    "# Limpeza do relatório de Junho\n",
    "if relatorio_fluxo_jun_df is not None:\n",
    "    relatorio_fluxo_jun_clean = relatorio_fluxo_jun_df.copy()\n",
    "    quantidade_cols = [col for col in relatorio_fluxo_jun_clean.columns if col.startswith('qtd_')]\n",
    "    for col in quantidade_cols:\n",
    "        relatorio_fluxo_jun_clean[col] = pd.to_numeric(relatorio_fluxo_jun_clean[col], errors='coerce')\n",
    "\n",
    "    if 'data' in relatorio_fluxo_jun_clean.columns:\n",
    "        relatorio_fluxo_jun_clean['data'] = pd.to_datetime(relatorio_fluxo_jun_clean['data'], errors='coerce')\n",
    "    if 'equipamento' in relatorio_fluxo_jun_clean.columns and relatorio_fluxo_jun_clean['equipamento'].dtype == 'object':\n",
    "        relatorio_fluxo_jun_clean['equipamento'] = relatorio_fluxo_jun_clean['equipamento'].astype(str).str.strip()\n",
    "    if 'faixa' in relatorio_fluxo_jun_clean.columns and relatorio_fluxo_jun_clean['faixa'].dtype == 'object':\n",
    "        relatorio_fluxo_jun_clean['faixa'] = relatorio_fluxo_jun_clean['faixa'].astype(str).str.strip()\n",
    "    if 'hora' in relatorio_fluxo_jun_clean.columns:\n",
    "        relatorio_fluxo_jun_clean['hora'] = pd.to_numeric(relatorio_fluxo_jun_clean['hora'], errors='coerce')\n",
    "    print(f\"   [OK] Relatório Junho limpo: {relatorio_fluxo_jun_clean.shape[0]} registros\")\n",
    "else:\n",
    "    relatorio_fluxo_jun_clean = None\n",
    "    print(\"   [ATENCAO]  Relatório Junho não disponível\")\n",
    "\n",
    "# Limpeza do relatório de Julho\n",
    "if relatorio_fluxo_jul_df is not None:\n",
    "    relatorio_fluxo_jul_clean = relatorio_fluxo_jul_df.copy()\n",
    "    quantidade_cols = [col for col in relatorio_fluxo_jul_clean.columns if col.startswith('qtd_')]\n",
    "    for col in quantidade_cols:\n",
    "        relatorio_fluxo_jul_clean[col] = pd.to_numeric(relatorio_fluxo_jul_clean[col], errors='coerce')\n",
    "\n",
    "    if 'data' in relatorio_fluxo_jul_clean.columns:\n",
    "        relatorio_fluxo_jul_clean['data'] = pd.to_datetime(relatorio_fluxo_jul_clean['data'], errors='coerce')\n",
    "    if 'equipamento' in relatorio_fluxo_jul_clean.columns and relatorio_fluxo_jul_clean['equipamento'].dtype == 'object':\n",
    "        relatorio_fluxo_jul_clean['equipamento'] = relatorio_fluxo_jul_clean['equipamento'].astype(str).str.strip()\n",
    "    if 'faixa' in relatorio_fluxo_jul_clean.columns and relatorio_fluxo_jul_clean['faixa'].dtype == 'object':\n",
    "        relatorio_fluxo_jul_clean['faixa'] = relatorio_fluxo_jul_clean['faixa'].astype(str).str.strip()\n",
    "    if 'hora' in relatorio_fluxo_jul_clean.columns:\n",
    "        relatorio_fluxo_jul_clean['hora'] = pd.to_numeric(relatorio_fluxo_jul_clean['hora'], errors='coerce')\n",
    "    print(f\"   [OK] Relatório Julho limpo: {relatorio_fluxo_jul_clean.shape[0]} registros\")\n",
    "else:\n",
    "    relatorio_fluxo_jul_clean = None\n",
    "    print(\"   [ATENCAO]  Relatório Julho não disponível\")\n",
    "\n",
    "# Limpeza do relatório de Agosto\n",
    "if relatorio_fluxo_ago_df is not None:\n",
    "    relatorio_fluxo_ago_clean = relatorio_fluxo_ago_df.copy()\n",
    "    quantidade_cols = [col for col in relatorio_fluxo_ago_clean.columns if col.startswith('qtd_')]\n",
    "    for col in quantidade_cols:\n",
    "        relatorio_fluxo_ago_clean[col] = pd.to_numeric(relatorio_fluxo_ago_clean[col], errors='coerce')\n",
    "\n",
    "    if 'data' in relatorio_fluxo_ago_clean.columns:\n",
    "        relatorio_fluxo_ago_clean['data'] = pd.to_datetime(relatorio_fluxo_ago_clean['data'], errors='coerce')\n",
    "    if 'equipamento' in relatorio_fluxo_ago_clean.columns and relatorio_fluxo_ago_clean['equipamento'].dtype == 'object':\n",
    "        relatorio_fluxo_ago_clean['equipamento'] = relatorio_fluxo_ago_clean['equipamento'].astype(str).str.strip()\n",
    "    if 'faixa' in relatorio_fluxo_ago_clean.columns and relatorio_fluxo_ago_clean['faixa'].dtype == 'object':\n",
    "        relatorio_fluxo_ago_clean['faixa'] = relatorio_fluxo_ago_clean['faixa'].astype(str).str.strip()\n",
    "    if 'hora' in relatorio_fluxo_ago_clean.columns:\n",
    "        relatorio_fluxo_ago_clean['hora'] = pd.to_numeric(relatorio_fluxo_ago_clean['hora'], errors='coerce')\n",
    "    print(f\"   [OK] Relatório Agosto limpo: {relatorio_fluxo_ago_clean.shape[0]} registros\")\n",
    "else:\n",
    "    relatorio_fluxo_ago_clean = None\n",
    "    print(\"   [ATENCAO]  Relatório Agosto não disponível\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2881f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. LIMPEZA DOS RELATÓRIOS DE FLUXO (AGOSTO E FEVEREIRO)\n",
    "print(\"[CONFIGURANDO] Limpando dados dos relatórios de fluxo...\")\n",
    "\n",
    "# Limpeza do relatório de Agosto\n",
    "relatorio_fluxo_ago_clean = relatorio_fluxo_ago_df.copy()\n",
    "quantidade_cols = [col for col in relatorio_fluxo_ago_clean.columns if col.startswith('qtd_')]\n",
    "for col in quantidade_cols:\n",
    "    relatorio_fluxo_ago_clean[col] = pd.to_numeric(relatorio_fluxo_ago_clean[col], errors='coerce')\n",
    "\n",
    "relatorio_fluxo_ago_clean['data'] = pd.to_datetime(relatorio_fluxo_ago_clean['data'], errors='coerce')\n",
    "relatorio_fluxo_ago_clean['equipamento'] = relatorio_fluxo_ago_clean['equipamento'].str.strip()\n",
    "relatorio_fluxo_ago_clean['faixa'] = relatorio_fluxo_ago_clean['faixa'].str.strip()\n",
    "relatorio_fluxo_ago_clean['hora'] = pd.to_numeric(relatorio_fluxo_ago_clean['hora'], errors='coerce')\n",
    "\n",
    "# Limpeza do relatório de Fevereiro\n",
    "relatorio_fluxo_fev_clean = relatorio_fluxo_fev_df.copy()\n",
    "for col in quantidade_cols:\n",
    "    relatorio_fluxo_fev_clean[col] = pd.to_numeric(relatorio_fluxo_fev_clean[col], errors='coerce')\n",
    "\n",
    "relatorio_fluxo_fev_clean['data'] = pd.to_datetime(relatorio_fluxo_fev_clean['data'], errors='coerce')\n",
    "relatorio_fluxo_fev_clean['equipamento'] = relatorio_fluxo_fev_clean['equipamento'].str.strip()\n",
    "relatorio_fluxo_fev_clean['faixa'] = relatorio_fluxo_fev_clean['faixa'].str.strip()\n",
    "relatorio_fluxo_fev_clean['hora'] = pd.to_numeric(relatorio_fluxo_fev_clean['hora'], errors='coerce')\n",
    "\n",
    "# Limpeza do relatório de Janeiro\n",
    "relatorio_fluxo_jan_clean = relatorio_fluxo_jan_df.copy()\n",
    "for col in quantidade_cols:\n",
    "    relatorio_fluxo_jan_clean[col] = pd.to_numeric(relatorio_fluxo_jan_clean[col], errors='coerce')\n",
    "\n",
    "relatorio_fluxo_jan_clean['data'] = pd.to_datetime(relatorio_fluxo_jan_clean['data'], errors='coerce')\n",
    "relatorio_fluxo_jan_clean['equipamento'] = relatorio_fluxo_jan_clean['equipamento'].str.strip()\n",
    "relatorio_fluxo_jan_clean['faixa'] = relatorio_fluxo_jan_clean['faixa'].str.strip()\n",
    "relatorio_fluxo_jan_clean['hora'] = pd.to_numeric(relatorio_fluxo_jan_clean['hour'], errors='coerce')\n",
    "\n",
    "# Limpeza do relatório de Março\n",
    "relatorio_fluxo_mar_clean = relatorio_fluxo_mar_df.copy()\n",
    "for col in quantidade_cols:\n",
    "    relatorio_fluxo_mar_clean[col] = pd.to_numeric(relatorio_fluxo_mar_clean[col], errors='coerce')\n",
    "\n",
    "relatorio_fluxo_mar_clean['data'] = pd.to_datetime(relatorio_fluxo_mar_clean['data'], errors='coerce')\n",
    "relatorio_fluxo_mar_clean['equipamento'] = relatorio_fluxo_mar_clean['equipamento'].str.strip()\n",
    "relatorio_fluxo_mar_clean['faixa'] = relatorio_fluxo_mar_clean['faixa'].str.strip()\n",
    "relatorio_fluxo_mar_clean['hora'] = pd.to_numeric(relatorio_fluxo_mar_clean['hora'], errors='coerce')\n",
    "\n",
    "# Limpeza do relatório de Abril\n",
    "relatorio_fluxo_abr_clean = relatorio_fluxo_abr_df.copy()\n",
    "for col in quantidade_cols:\n",
    "    relatorio_fluxo_abr_clean[col] = pd.to_numeric(relatorio_fluxo_abr_clean[col], errors='coerce')\n",
    "\n",
    "relatorio_fluxo_abr_clean['data'] = pd.to_datetime(relatorio_fluxo_abr_clean['data'], errors='coerce')\n",
    "relatorio_fluxo_abr_clean['equipamento'] = relatorio_fluxo_abr_clean['equipamento'].str.strip()\n",
    "relatorio_fluxo_abr_clean['faixa'] = relatorio_fluxo_abr_clean['faixa'].str.strip()\n",
    "relatorio_fluxo_abr_clean['hora'] = pd.to_numeric(relatorio_fluxo_abr_clean['hora'], errors='coerce')\n",
    "\n",
    "# Limpeza do relatório de Maio\n",
    "relatorio_fluxo_mai_clean = relatorio_fluxo_mai_df.copy()\n",
    "for col in quantidade_cols:\n",
    "    relatorio_fluxo_mai_clean[col] = pd.to_numeric(relatorio_fluxo_mai_clean[col], errors='coerce')\n",
    "\n",
    "relatorio_fluxo_mai_clean['data'] = pd.to_datetime(relatorio_fluxo_mai_clean['data'], errors='coerce')\n",
    "relatorio_fluxo_mai_clean['equipamento'] = relatorio_fluxo_mai_clean['equipamento'].astype(str).str.strip()\n",
    "relatorio_fluxo_mai_clean['faixa'] = relatorio_fluxo_mai_clean['faixa'].str.strip()\n",
    "relatorio_fluxo_mai_clean['hora'] = pd.to_numeric(relatorio_fluxo_mai_clean['hora'], errors='coerce')\n",
    "\n",
    "# Limpeza do relatório de Junho\n",
    "relatorio_fluxo_jun_clean = relatorio_fluxo_jun_df.copy()\n",
    "for col in quantidade_cols:\n",
    "    relatorio_fluxo_jun_clean[col] = pd.to_numeric(relatorio_fluxo_jun_clean[col], errors='coerce')\n",
    "\n",
    "relatorio_fluxo_jun_clean['data'] = pd.to_datetime(relatorio_fluxo_jun_clean['data'], errors='coerce')\n",
    "relatorio_fluxo_jun_clean['equipamento'] = relatorio_fluxo_jun_clean['equipamento'].str.strip()\n",
    "relatorio_fluxo_jun_clean['faixa'] = relatorio_fluxo_jun_clean['faixa'].str.strip()\n",
    "relatorio_fluxo_jun_clean['hora'] = pd.to_numeric(relatorio_fluxo_jun_clean['hora'], errors='coerce')\n",
    "\n",
    "# Limpeza do relatório de Julho\n",
    "relatorio_fluxo_jul_clean = relatorio_fluxo_jul_df.copy()\n",
    "for col in quantidade_cols:\n",
    "    relatorio_fluxo_jul_clean[col] = pd.to_numeric(relatorio_fluxo_jul_clean[col], errors='coerce')\n",
    "\n",
    "relatorio_fluxo_jul_clean['data'] = pd.to_datetime(relatorio_fluxo_jul_clean['data'], errors='coerce')\n",
    "relatorio_fluxo_jul_clean['equipamento'] = relatorio_fluxo_jul_clean['equipamento'].str.strip()\n",
    "relatorio_fluxo_jul_clean['faixa'] = relatorio_fluxo_jul_clean['faixa'].str.strip()\n",
    "relatorio_fluxo_jul_clean['hora'] = pd.to_numeric(relatorio_fluxo_jul_clean['hora'], errors='coerce')\n",
    "\n",
    "print(f\"   [OK] Relatório Fevereiro limpo: {relatorio_fluxo_jan_clean.shape[0]} registros\")\n",
    "print(f\"   [OK] Relatório Fevereiro limpo: {relatorio_fluxo_fev_clean.shape[0]} registros\")\n",
    "print(f\"   [OK] Relatório Fevereiro limpo: {relatorio_fluxo_mar_clean.shape[0]} registros\")\n",
    "print(f\"   [OK] Relatório Fevereiro limpo: {relatorio_fluxo_abr_clean.shape[0]} registros\")\n",
    "print(f\"   [OK] Relatório Fevereiro limpo: {relatorio_fluxo_mai_clean.shape[0]} registros\")\n",
    "print(f\"   [OK] Relatório Fevereiro limpo: {relatorio_fluxo_jun_clean.shape[0]} registros\")\n",
    "print(f\"   [OK] Relatório Fevereiro limpo: {relatorio_fluxo_jul_clean.shape[0]} registros\")\n",
    "print(f\"   [OK] Relatório Agosto limpo: {relatorio_fluxo_ago_clean.shape[0]} registros\")\n",
    "\n",
    "print(f\"   [OK] Valores nulos Fevereiro: {relatorio_fluxo_jan_clean.isnull().sum().sum()}\")\n",
    "print(f\"   [OK] Valores nulos Fevereiro: {relatorio_fluxo_fev_clean.isnull().sum().sum()}\")\n",
    "print(f\"   [OK] Valores nulos Fevereiro: {relatorio_fluxo_mar_clean.isnull().sum().sum()}\")\n",
    "print(f\"   [OK] Valores nulos Fevereiro: {relatorio_fluxo_abr_clean.isnull().sum().sum()}\")\n",
    "print(f\"   [OK] Valores nulos Fevereiro: {relatorio_fluxo_mai_clean.isnull().sum().sum()}\")\n",
    "print(f\"   [OK] Valores nulos Fevereiro: {relatorio_fluxo_jun_clean.isnull().sum().sum()}\")\n",
    "print(f\"   [OK] Valores nulos Fevereiro: {relatorio_fluxo_jul_clean.isnull().sum().sum()}\")\n",
    "print(f\"   [OK] Valores nulos Agosto: {relatorio_fluxo_ago_clean.isnull().sum().sum()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0cc9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANÁLISES EXPLORATÓRIAS E ESTATÍSTICAS\n",
    "\n",
    "print(\"=== ANÁLISES EXPLORATÓRIAS ===\\n\")\n",
    "\n",
    "# 1. ANÁLISE DOS SEMÁFOROS\n",
    "if semaforos_clean is not None:\n",
    "    print(\"[SINAL] ANÁLISE DOS SEMÁFOROS:\")\n",
    "    print(f\"   - Total de semáforos: {len(semaforos_clean)}\")\n",
    "    print(f\"   - Bairros únicos: {semaforos_clean['bairro'].nunique()}\")\n",
    "    print(f\"   - Tipos de funcionamento: {semaforos_clean['funcionamento'].value_counts().to_dict()}\")\n",
    "    print(f\"   - Coordenadas válidas: {semaforos_clean[['latitude', 'longitude']].notna().all(axis=1).sum()}\")\n",
    "else:\n",
    "    print(\"[SINAL] SEMÁFOROS: Não disponível\")\n",
    "\n",
    "# 2. ANÁLISE DOS EQUIPAMENTOS DE MEDIÇÃO\n",
    "if equip_med_vel_clean is not None:\n",
    "    print(\"\\n[ANALISE] ANÁLISE DOS EQUIPAMENTOS DE MEDIÇÃO:\")\n",
    "    print(f\"   - Total de equipamentos: {len(equip_med_vel_clean)}\")\n",
    "    print(f\"   - Colunas disponíveis: {list(equip_med_vel_clean.columns)}\")\n",
    "    \n",
    "    # Análises condicionais baseadas nas colunas disponíveis\n",
    "    if 'velocidade_fiscalizada' in equip_med_vel_clean.columns:\n",
    "        print(f\"   - Velocidade fiscalizada média: {equip_med_vel_clean['velocidade_fiscalizada'].mean():.1f} km/h\")\n",
    "    if 'vmd' in equip_med_vel_clean.columns:\n",
    "        print(f\"   - VMD médio: {equip_med_vel_clean['vmd'].mean():.0f} veículos\")\n",
    "    if 'faixas_fiscalizadas' in equip_med_vel_clean.columns:\n",
    "        print(f\"   - Faixas fiscalizadas: {equip_med_vel_clean['faixas_fiscalizadas'].value_counts().to_dict()}\")\n",
    "else:\n",
    "    print(\"\\n[ANALISE] EQUIPAMENTOS DE MEDIÇÃO: Não disponível\")\n",
    "\n",
    "# 3. ANÁLISE DO FLUXO DE VEÍCULOS\n",
    "if fluxo_veiculos_hora_clean is not None:\n",
    "    print(\"\\n[VEICULO] ANÁLISE DO FLUXO DE VEÍCULOS:\")\n",
    "    print(f\"   - Registros de fluxo por hora: {len(fluxo_veiculos_hora_clean)}\")\n",
    "    print(f\"   - Equipamentos únicos: {fluxo_veiculos_hora_clean['equipamento'].nunique()}\")\n",
    "    print(f\"   - Total de veículos registrados: {fluxo_veiculos_hora_clean['quanttotal'].sum():,.0f}\")\n",
    "else:\n",
    "    print(\"\\n[VEICULO] FLUXO DE VEÍCULOS: Não disponível\")\n",
    "\n",
    "# 4. ANÁLISE DOS DADOS DE VELOCIDADE EM 15 MINUTOS\n",
    "if fluxo_velocidade_15min_clean is not None:\n",
    "    print(\"\\n[RAPIDO] ANÁLISE DE VELOCIDADE EM 15 MINUTOS:\")\n",
    "    print(f\"   - Registros de velocidade: {len(fluxo_velocidade_15min_clean)}\")\n",
    "    print(f\"   - Período dos dados: {fluxo_velocidade_15min_clean['data'].min()} a {fluxo_velocidade_15min_clean['data'].max()}\")\n",
    "    print(f\"   - Equipamentos únicos: {fluxo_velocidade_15min_clean['equipamento'].nunique()}\")\n",
    "else:\n",
    "    print(\"\\n[RAPIDO] VELOCIDADE EM 15 MINUTOS: Não disponível\")\n",
    "\n",
    "# 5. ANÁLISE DOS RELATÓRIOS DE FLUXO\n",
    "if relatorio_fluxo_jan_clean is not None and relatorio_fluxo_fev_clean is not None and relatorio_fluxo_mar_clean is not None and relatorio_fluxo_abr_clean is not None and relatorio_fluxo_mai_clean is not None and relatorio_fluxo_jun_clean is not None and relatorio_fluxo_jul_clean is not None and relatorio_fluxo_ago_clean is not None:\n",
    "    print(\"\\n[CRESCIMENTO] ANÁLISE DOS RELATÓRIOS DE FLUXO:\")\n",
    "    print(f\"   - Registros Janeiro 2025: {len(relatorio_fluxo_jan_clean)}\")\n",
    "    print(f\"   - Registros Fevereiro 2025: {len(relatorio_fluxo_fev_clean)}\")\n",
    "    print(f\"   - Registros Março 2025: {len(relatorio_fluxo_mar_clean)}\")\n",
    "    print(f\"   - Registros Abril 2025: {len(relatorio_fluxo_abr_clean)}\")\n",
    "    print(f\"   - Registros Maio 2025: {len(relatorio_fluxo_mai_clean)}\")\n",
    "    print(f\"   - Registros Junho 2025: {len(relatorio_fluxo_jun_clean)}\")\n",
    "    print(f\"   - Registros Julho 2025: {len(relatorio_fluxo_jul_clean)}\")\n",
    "    print(f\"   - Registros Agosto 2025: {len(relatorio_fluxo_ago_clean)}\")\n",
    "    print(f\"   - Total de registros: {len(relatorio_fluxo_ago_clean) + len(relatorio_fluxo_fev_clean) + len(relatorio_fluxo_jan_clean) + len(relatorio_fluxo_mar_clean) + len(relatorio_fluxo_abr_clean) + len(relatorio_fluxo_mai_clean) + len(relatorio_fluxo_jun_clean) + len(relatorio_fluxo_jul_clean)}\")\n",
    "else:\n",
    "    print(\"\\n[CRESCIMENTO] RELATÓRIOS DE FLUXO: Parcialmente disponível\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae909614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SALVAR DADOS PROCESSADOS\n",
    "\n",
    "print(\"=== SALVANDO DADOS PROCESSADOS ===\\n\")\n",
    "\n",
    "import os\n",
    "\n",
    "# Criar diretório de dados processados se não existir\n",
    "data_processed_path = os.path.join(project_root, \"data\", \"processed\")\n",
    "os.makedirs(data_processed_path, exist_ok=True)\n",
    "\n",
    "# Salvar apenas os datasets que foram processados\n",
    "datasets_clean = {\n",
    "    \"semaforos_clean.csv\": semaforos_clean,\n",
    "    \"equipamentos_medicao_velocidade_clean.csv\": equip_med_vel_clean,\n",
    "    \"fluxo_veiculos_hora_clean.csv\": fluxo_veiculos_hora_clean,\n",
    "    \"fluxo_velocidade_15min_clean.csv\": fluxo_velocidade_15min_clean,\n",
    "    \"monitoramento_cttu_clean.csv\": monitoramento_cttu_df,  # Este não foi limpo ainda\n",
    "    \"relatorio_fluxo_janeiro_2025_clean.csv\": relatorio_fluxo_jan_clean,\n",
    "    \"relatorio_fluxo_fevereiro_2025_clean.csv\": relatorio_fluxo_fev_clean,\n",
    "    \"relatorio_fluxo_marco_2025_clean.csv\": relatorio_fluxo_mar_clean,\n",
    "    \"relatorio_fluxo_abril_2025_clean.csv\": relatorio_fluxo_abr_clean,\n",
    "    \"relatorio_fluxo_maio_2025_clean.csv\": relatorio_fluxo_mai_clean,\n",
    "    \"relatorio_fluxo_junho_2025_clean.csv\": relatorio_fluxo_jun_clean,\n",
    "    \"relatorio_fluxo_julho_2025_clean.csv\": relatorio_fluxo_jul_clean,\n",
    "    \"relatorio_fluxo_agosto_2025_clean.csv\": relatorio_fluxo_ago_clean\n",
    "}\n",
    "\n",
    "# Filtrar apenas datasets que não são None\n",
    "datasets_para_salvar = {k: v for k, v in datasets_clean.items() if v is not None}\n",
    "\n",
    "for filename, df in datasets_para_salvar.items():\n",
    "    filepath = os.path.join(data_processed_path, filename)\n",
    "    df.to_csv(filepath, index=False, encoding='utf-8')\n",
    "    print(f\"[OK] Salvo: {filename} ({df.shape[0]} registros)\")\n",
    "\n",
    "# Mostrar quais datasets não foram salvos\n",
    "datasets_nao_salvos = {k: v for k, v in datasets_clean.items() if v is None}\n",
    "if datasets_nao_salvos:\n",
    "    print(f\"\\n[ATENCAO]  Datasets não salvos (não disponíveis): {list(datasets_nao_salvos.keys())}\")\n",
    "\n",
    "print(f\"\\n[SUCESSO] Dados processados salvos em '{data_processed_path}'\")\n",
    "print(f\"[ARQUIVO] Total de arquivos salvos: {len(datasets_para_salvar)}\")\n",
    "\n",
    "# Resumo final\n",
    "print(\"\\n=== RESUMO FINAL ===\")\n",
    "total_registros = sum(df.shape[0] for df in datasets_para_salvar.values())\n",
    "print(f\"[ANALISE] Total de registros processados: {total_registros:,}\")\n",
    "print(f\"[ARMAZENANDO] Tamanho total estimado: {sum(df.memory_usage(deep=True).sum() for df in datasets_para_salvar.values()) / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce384dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GERAÇÃO AUTOMÁTICA DE SCHEMAS SQL\n",
    "\n",
    "print(\"=== GERAÇÃO AUTOMÁTICA DE SCHEMAS SQL ===\\n\")\n",
    "\n",
    "def generate_sql_schema(df, table_name, primary_key=None):\n",
    "    \"\"\"Gera schema SQL automaticamente baseado no DataFrame\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "    \n",
    "    schema_lines = [f\"CREATE TABLE IF NOT EXISTS {table_name} (\"]\n",
    "    \n",
    "    # Adicionar colunas\n",
    "    for i, (col, dtype) in enumerate(df.dtypes.items()):\n",
    "        # Converter tipos do pandas para SQL\n",
    "        if dtype == 'int64':\n",
    "            sql_type = \"INTEGER\"\n",
    "        elif dtype == 'float64':\n",
    "            sql_type = \"DECIMAL(10, 2)\"\n",
    "        elif dtype == 'object':\n",
    "            # Para strings, verificar tamanho máximo\n",
    "            try:\n",
    "                max_len = df[col].astype(str).str.len().max()\n",
    "                if max_len > 255:\n",
    "                    sql_type = \"TEXT\"\n",
    "                else:\n",
    "                    sql_type = f\"VARCHAR({max(255, max_len)})\"\n",
    "            except:\n",
    "                sql_type = \"TEXT\"\n",
    "        elif 'datetime' in str(dtype):\n",
    "            sql_type = \"TIMESTAMP\"\n",
    "        else:\n",
    "            sql_type = \"TEXT\"\n",
    "        \n",
    "        # CORREÇÕES ESPECÍFICAS PARA TIPOS DE DADOS\n",
    "        # Coordenadas com precisão correta\n",
    "        if col in ['latitude']:\n",
    "            sql_type = \"DECIMAL(10, 8)\"\n",
    "        elif col in ['longitude']:\n",
    "            sql_type = \"DECIMAL(11, 8)\"\n",
    "        \n",
    "        # Horários como TIME em vez de TIMESTAMP\n",
    "        if col in ['horainicio', 'horafinal']:\n",
    "            sql_type = \"TIME\"\n",
    "        \n",
    "        # Datas como DATE em vez de TIMESTAMP\n",
    "        if col == 'data' and 'datetime' in str(dtype):\n",
    "            sql_type = \"DATE\"\n",
    "        \n",
    "        # Adicionar PRIMARY KEY se especificado\n",
    "        if primary_key and col == primary_key:\n",
    "            if col == '_id':\n",
    "                sql_type += \" PRIMARY KEY\"  # INTEGER PRIMARY KEY\n",
    "            else:\n",
    "                sql_type += \" SERIAL PRIMARY KEY\"  # SERIAL PRIMARY KEY\n",
    "        elif not primary_key and i == 0:\n",
    "            sql_type += \" SERIAL PRIMARY KEY\"\n",
    "        \n",
    "        col_name = col.replace('-', '_')\n",
    "        schema_lines.append(f\"    {col_name} {sql_type},\")\n",
    "    \n",
    "    # Adicionar campo created_at\n",
    "    schema_lines.append(\"    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\")\n",
    "    schema_lines.append(\");\")\n",
    "    \n",
    "    return \"\\n\".join(schema_lines)\n",
    "\n",
    "# Gerar schemas para todos os datasets processados\n",
    "print(\"[RESUMO] Gerando schemas SQL automaticamente...\")\n",
    "\n",
    "schemas_generated = {}\n",
    "\n",
    "# 1. Semáforos - usar _id como PRIMARY KEY\n",
    "if semaforos_clean is not None:\n",
    "    schema = generate_sql_schema(semaforos_clean, \"semaforos\", \"_id\")\n",
    "    if schema:\n",
    "        schemas_generated[\"semaforos_schema.sql\"] = schema\n",
    "        print(\"[OK] Schema gerado: semaforos\")\n",
    "\n",
    "# 2. Equipamentos de medição - usar _id como PRIMARY KEY\n",
    "if equip_med_vel_clean is not None:\n",
    "    schema = generate_sql_schema(equip_med_vel_clean, \"equipamentos_medicao_velocidade\", \"_id\")\n",
    "    if schema:\n",
    "        schemas_generated[\"equipamentos_medicao_schema.sql\"] = schema\n",
    "        print(\"[OK] Schema gerado: equipamentos_medicao_velocidade\")\n",
    "\n",
    "# 3. Fluxo de veículos por hora - usar id SERIAL\n",
    "if fluxo_veiculos_hora_clean is not None:\n",
    "    schema = generate_sql_schema(fluxo_veiculos_hora_clean, \"fluxo_veiculos_hora\", \"id\")\n",
    "    if schema:\n",
    "        schemas_generated[\"fluxo_veiculos_hora_schema.sql\"] = schema\n",
    "        print(\"[OK] Schema gerado: fluxo_veiculos_hora\")\n",
    "\n",
    "# 4. Fluxo e velocidade 15min - usar id SERIAL\n",
    "if fluxo_velocidade_15min_clean is not None:\n",
    "    schema = generate_sql_schema(fluxo_velocidade_15min_clean, \"fluxo_velocidade_15min\", \"id\")\n",
    "    if schema:\n",
    "        schemas_generated[\"fluxo_velocidade_15min_schema.sql\"] = schema\n",
    "        print(\"[OK] Schema gerado: fluxo_velocidade_15min\")\n",
    "\n",
    "# 5. Monitoramento CTTU - usar id SERIAL\n",
    "if monitoramento_cttu_df is not None:\n",
    "    schema = generate_sql_schema(monitoramento_cttu_df, \"monitoramento_cttu\", \"id\")\n",
    "    if schema:\n",
    "        schemas_generated[\"monitoramento_cttu_schema.sql\"] = schema\n",
    "        print(\"[OK] Schema gerado: monitoramento_cttu\")\n",
    "\n",
    "# 6. Relatório fluxo Agosto - usar id SERIAL\n",
    "if relatorio_fluxo_ago_clean is not None:\n",
    "    schema = generate_sql_schema(relatorio_fluxo_ago_clean, \"relatorio_fluxo_agosto\", \"id\")\n",
    "    if schema:\n",
    "        schemas_generated[\"relatorio_fluxo_agosto_schema.sql\"] = schema\n",
    "        print(\"[OK] Schema gerado: relatorio_fluxo_agosto\")\n",
    "\n",
    "# 7. Relatório fluxo Fevereiro - usar id SERIAL\n",
    "if relatorio_fluxo_fev_clean is not None:\n",
    "    schema = generate_sql_schema(relatorio_fluxo_fev_clean, \"relatorio_fluxo_fevereiro\", \"id\")\n",
    "    if schema:\n",
    "        schemas_generated[\"relatorio_fluxo_fevereiro_schema.sql\"] = schema\n",
    "        print(\"[OK] Schema gerado: relatorio_fluxo_fevereiro\")\n",
    "\n",
    "if relatorio_fluxo_jan_clean is not None:\n",
    "    schema = generate_sql_schema(relatorio_fluxo_jan_clean, \"relatorio_fluxo_janeiro\", \"id\")\n",
    "    if schema:\n",
    "        schemas_generated[\"relatorio_fluxo_janeiro_schema.sql\"] = schema\n",
    "        print(\"[OK] Schema gerado: relatorio_fluxo_janeiro\")\n",
    "\n",
    "if relatorio_fluxo_mar_clean is not None:\n",
    "    schema = generate_sql_schema(relatorio_fluxo_mar_clean, \"relatorio_fluxo_marco\", \"id\")\n",
    "    if schema:\n",
    "        schemas_generated[\"relatorio_fluxo_marco_schema.sql\"] = schema\n",
    "        print(\"[OK] Schema gerado: relatorio_fluxo_marco\")\n",
    "\n",
    "if relatorio_fluxo_abr_clean is not None:\n",
    "    schema = generate_sql_schema(relatorio_fluxo_abr_clean, \"relatorio_fluxo_abril\", \"id\")\n",
    "    if schema:\n",
    "        schemas_generated[\"relatorio_fluxo_abril_schema.sql\"] = schema\n",
    "        print(\"[OK] Schema gerado: relatorio_fluxo_abril\")\n",
    "\n",
    "if relatorio_fluxo_mai_clean is not None:\n",
    "    schema = generate_sql_schema(relatorio_fluxo_mai_clean, \"relatorio_fluxo_maio\", \"id\")\n",
    "    if schema:\n",
    "        schemas_generated[\"relatorio_fluxo_maio_schema.sql\"] = schema\n",
    "        print(\"[OK] Schema gerado: relatorio_fluxo_maio\")\n",
    "\n",
    "if relatorio_fluxo_jun_clean is not None:\n",
    "    schema = generate_sql_schema(relatorio_fluxo_jun_clean, \"relatorio_fluxo_junho\", \"id\")\n",
    "    if schema:\n",
    "        schemas_generated[\"relatorio_fluxo_junho_schema.sql\"] = schema\n",
    "        print(\"[OK] Schema gerado: relatorio_fluxo_junho\")\n",
    "\n",
    "if relatorio_fluxo_jul_clean is not None:\n",
    "    schema = generate_sql_schema(relatorio_fluxo_jul_clean, \"relatorio_fluxo_julho\", \"id\")\n",
    "    if schema:\n",
    "        schemas_generated[\"relatorio_fluxo_julho_schema.sql\"] = schema\n",
    "        print(\"[OK] Schema gerado: relatorio_fluxo_julho\")\n",
    "        \n",
    "# Salvar schemas em arquivos\n",
    "if schemas_generated:\n",
    "    database_schemas_path = os.path.join(project_root, \"database\", \"schemas\")\n",
    "    os.makedirs(database_schemas_path, exist_ok=True)\n",
    "    \n",
    "    for filename, schema in schemas_generated.items():\n",
    "        filepath = os.path.join(database_schemas_path, filename)\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(schema)\n",
    "        print(f\"[ARMAZENANDO] Schema salvo: {filename}\")\n",
    "    \n",
    "    print(f\"\\n[SUCESSO] {len(schemas_generated)} schemas SQL gerados automaticamente!\")\n",
    "    print(f\"[ARQUIVO] Salvos em: {database_schemas_path}\")\n",
    "else:\n",
    "    print(\"[ATENCAO]  Nenhum dataset processado encontrado para gerar schemas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7584d08",
   "metadata": {},
   "source": [
    "### Limpando faixaazul.geojson para Grafana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4980c3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import geopandas as gpd\n",
    "from http.server import SimpleHTTPRequestHandler, HTTPServer\n",
    "import threading\n",
    "import os\n",
    "\n",
    "geojson_path = \"../../data/raw/faixaazul.geojson\"\n",
    "\n",
    "# Ler e validar o .geojson\n",
    "try:\n",
    "    gdf = gpd.read_file(geojson_path)\n",
    "    print(\"GeoJSON carregado com sucesso!\")\n",
    "    print(\"Número de faixas:\", len(gdf))\n",
    "except Exception as e:\n",
    "    print(\"Erro ao ler o GeoJSON:\", e)\n",
    "\n",
    "# Garantir que o CRS está em WGS84 (EPSG:4326)\n",
    "if gdf.crs is None or gdf.crs.to_string() != \"EPSG:4326\":\n",
    "    gdf = gdf.to_crs(epsg=4326)\n",
    "    print(\"Sistema de coordenadas ajustado para WGS84 (EPSG:4326)\")\n",
    "\n",
    "# Salvar uma versão \"limpa\" do GeoJSON\n",
    "clean_path = \"../../data/processed/faixaazul_clean.geojson\"\n",
    "gdf.to_file(clean_path, driver=\"GeoJSON\")\n",
    "print(f\"Arquivo salvo: {clean_path}\")\n",
    "\n",
    "# Servidor HTTP para expor o arquivo\n",
    "\n",
    "# Função para iniciar servidor simples\n",
    "def start_server(directory=\".\", port=8080):\n",
    "    os.chdir(directory)\n",
    "    handler = SimpleHTTPRequestHandler\n",
    "    httpd = HTTPServer((\"\", port), handler)\n",
    "    print(f\"Servidor rodando em http://localhost:{port}/\")\n",
    "    print(f\"Acesse no Grafana: http://localhost:{port}/{os.path.basename(clean_path)}\")\n",
    "    httpd.serve_forever()\n",
    "\n",
    "# Rodar o servidor em thread paralela\n",
    "thread = threading.Thread(target=start_server, args=(\".\", 8080))\n",
    "thread.daemon = True\n",
    "thread.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
